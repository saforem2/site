---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: RG inspired Machine Learning for lattice field theory
subtitle: ''
summary: ''
authors:
- S. Foreman
- J. Giedt
- Y. Meurice
- J. Unmuth-Yockey
tags:
- '"Condensed Matter - Statistical Mechanics"'
- '"High Energy Physics - Lattice"'
categories: []
date: '2017-10-05'
lastmod: 2021-07-20T17:30:07-05:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-07-20T22:30:07.367637Z'
publication_types:
- '2'
abstract: Machine learning has been a fast growing field of research in several areas
  dealing with large datasets. We report recent attempts to use Renormalization Group
  (RG) ideas in the context of machine learning. We examine coarse graining procedures
  for perceptron models designed to identify the digits of the MNIST data. We discuss
  the correspondence between principal components analysis (PCA) and RG flows across
  the transition for worm configurations of the 2D Ising model. Preliminary results
  regarding the logarithmic divergence of the leading PCA eigenvalue were presented
  at the conference and have been improved after. More generally, we discuss the relationship
  between PCA and observables in Monte Carlo simulations and the possibility of reduction
  of the number of learning parameters in supervised learning based on RG inspired
  hierarchical ansatzes.
publication: ''
url_pdf: http://arxiv.org/abs/1710.02079
---
